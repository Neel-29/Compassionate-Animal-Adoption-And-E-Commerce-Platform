{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMcBHXf7H0+ZbIy5GIpxnsJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Neel-29/Compassionate-Animal-Adoption-And-E-Commerce-Platform/blob/main/test1/HSI_XML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tLV3yakiRsiw",
        "outputId": "a03a237f-5664-4597-932b-ffadad6a2bc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%%**********************]  1 of 1 completed\n",
            "<ipython-input-1-c30efe3c131e>:23: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  data['Close'] = data['Close'].fillna(method='ffill')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "95/95 [==============================] - 3s 10ms/step - loss: 486858208.0000 - mae: 21627.6523 - val_loss: 367098048.0000 - val_mae: 18622.5469\n",
            "Epoch 2/15\n",
            "95/95 [==============================] - 1s 8ms/step - loss: 129063688.0000 - mae: 9976.6074 - val_loss: 59011548.0000 - val_mae: 6941.3179\n",
            "Epoch 3/15\n",
            "95/95 [==============================] - 1s 9ms/step - loss: 31106152.0000 - mae: 4829.4224 - val_loss: 13284581.0000 - val_mae: 3108.2283\n",
            "Epoch 4/15\n",
            "95/95 [==============================] - 1s 8ms/step - loss: 7591286.0000 - mae: 2147.3625 - val_loss: 3620887.0000 - val_mae: 1468.2675\n",
            "Epoch 5/15\n",
            "95/95 [==============================] - 1s 12ms/step - loss: 2603897.2500 - mae: 1156.4015 - val_loss: 1491037.3750 - val_mae: 903.5805\n",
            "Epoch 6/15\n",
            "95/95 [==============================] - 1s 10ms/step - loss: 1242091.5000 - mae: 747.7725 - val_loss: 793810.4375 - val_mae: 619.5690\n",
            "Epoch 7/15\n",
            "95/95 [==============================] - 0s 3ms/step - loss: 718441.1250 - mae: 549.7934 - val_loss: 523075.7188 - val_mae: 514.6255\n",
            "Epoch 8/15\n",
            "95/95 [==============================] - 0s 3ms/step - loss: 471372.9062 - mae: 446.0475 - val_loss: 368220.5000 - val_mae: 411.9025\n",
            "Epoch 9/15\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 343053.3750 - mae: 384.4929 - val_loss: 279501.3438 - val_mae: 368.4743\n",
            "Epoch 10/15\n",
            "95/95 [==============================] - 0s 3ms/step - loss: 266354.8125 - mae: 344.5828 - val_loss: 240014.2500 - val_mae: 350.0047\n",
            "Epoch 11/15\n",
            "95/95 [==============================] - 0s 3ms/step - loss: 219708.8750 - mae: 318.9305 - val_loss: 200541.2969 - val_mae: 318.3530\n",
            "Epoch 12/15\n",
            "95/95 [==============================] - 0s 3ms/step - loss: 187048.1875 - mae: 300.9604 - val_loss: 181233.3906 - val_mae: 306.0176\n",
            "Epoch 13/15\n",
            "95/95 [==============================] - 0s 3ms/step - loss: 167309.8281 - mae: 288.6014 - val_loss: 169603.7188 - val_mae: 296.1496\n",
            "Epoch 14/15\n",
            "95/95 [==============================] - 0s 3ms/step - loss: 153284.6562 - mae: 279.6103 - val_loss: 158256.1406 - val_mae: 288.3819\n",
            "Epoch 15/15\n",
            "95/95 [==============================] - 0s 2ms/step - loss: 144728.5469 - mae: 272.3745 - val_loss: 150547.2500 - val_mae: 281.4961\n",
            "Epoch 1/50\n",
            "117/117 [==============================] - 10s 56ms/step - loss: 0.0106\n",
            "Epoch 2/50\n",
            "117/117 [==============================] - 5s 44ms/step - loss: 8.9522e-04\n",
            "Epoch 3/50\n",
            "117/117 [==============================] - 5s 46ms/step - loss: 8.3609e-04\n",
            "Epoch 4/50\n",
            "117/117 [==============================] - 6s 54ms/step - loss: 8.0228e-04\n",
            "Epoch 5/50\n",
            "117/117 [==============================] - 5s 44ms/step - loss: 6.9892e-04\n",
            "Epoch 6/50\n",
            "117/117 [==============================] - 7s 57ms/step - loss: 6.5398e-04\n",
            "Epoch 7/50\n",
            "117/117 [==============================] - 5s 44ms/step - loss: 6.3200e-04\n",
            "Epoch 8/50\n",
            "117/117 [==============================] - 7s 56ms/step - loss: 6.1049e-04\n",
            "Epoch 9/50\n",
            "117/117 [==============================] - 5s 44ms/step - loss: 5.3688e-04\n",
            "Epoch 10/50\n",
            "117/117 [==============================] - 5s 47ms/step - loss: 5.1276e-04\n",
            "Epoch 11/50\n",
            "117/117 [==============================] - 6s 55ms/step - loss: 4.6264e-04\n",
            "Epoch 12/50\n",
            "117/117 [==============================] - 6s 50ms/step - loss: 4.4264e-04\n",
            "Epoch 13/50\n",
            "117/117 [==============================] - 8s 68ms/step - loss: 4.2018e-04\n",
            "Epoch 14/50\n",
            "117/117 [==============================] - 5s 44ms/step - loss: 4.0530e-04\n",
            "Epoch 15/50\n",
            "117/117 [==============================] - 7s 58ms/step - loss: 4.1913e-04\n",
            "Epoch 16/50\n",
            "117/117 [==============================] - 5s 45ms/step - loss: 3.6188e-04\n",
            "Epoch 17/50\n",
            "117/117 [==============================] - 10s 90ms/step - loss: 3.6505e-04\n",
            "Epoch 18/50\n",
            "117/117 [==============================] - 13s 113ms/step - loss: 3.6089e-04\n",
            "Epoch 19/50\n",
            "117/117 [==============================] - 13s 108ms/step - loss: 3.3947e-04\n",
            "Epoch 20/50\n",
            "117/117 [==============================] - 5s 45ms/step - loss: 3.2661e-04\n",
            "Epoch 21/50\n",
            "117/117 [==============================] - 6s 50ms/step - loss: 3.1908e-04\n",
            "Epoch 22/50\n",
            "117/117 [==============================] - 6s 51ms/step - loss: 3.2271e-04\n",
            "Epoch 23/50\n",
            "117/117 [==============================] - 5s 45ms/step - loss: 2.9318e-04\n",
            "Epoch 24/50\n",
            "117/117 [==============================] - 7s 58ms/step - loss: 2.7814e-04\n",
            "Epoch 25/50\n",
            "117/117 [==============================] - 5s 44ms/step - loss: 2.9442e-04\n",
            "Epoch 26/50\n",
            "117/117 [==============================] - 6s 55ms/step - loss: 2.5967e-04\n",
            "Epoch 27/50\n",
            "117/117 [==============================] - 5s 45ms/step - loss: 2.6304e-04\n",
            "Epoch 28/50\n",
            "117/117 [==============================] - 6s 52ms/step - loss: 2.7158e-04\n",
            "Epoch 29/50\n",
            "117/117 [==============================] - 6s 50ms/step - loss: 2.4931e-04\n",
            "Epoch 30/50\n",
            "117/117 [==============================] - 5s 45ms/step - loss: 2.5772e-04\n",
            "Epoch 31/50\n",
            "117/117 [==============================] - 7s 58ms/step - loss: 2.7274e-04\n",
            "Epoch 32/50\n",
            "117/117 [==============================] - 5s 45ms/step - loss: 2.4450e-04\n",
            "Epoch 33/50\n",
            "117/117 [==============================] - 7s 57ms/step - loss: 2.2917e-04\n",
            "Epoch 34/50\n",
            "117/117 [==============================] - 5s 45ms/step - loss: 2.3190e-04\n",
            "Epoch 35/50\n",
            "117/117 [==============================] - 6s 54ms/step - loss: 2.4038e-04\n",
            "Epoch 36/50\n",
            "117/117 [==============================] - 7s 61ms/step - loss: 2.2244e-04\n",
            "Epoch 37/50\n",
            "117/117 [==============================] - 6s 54ms/step - loss: 2.2681e-04\n",
            "Epoch 38/50\n",
            "117/117 [==============================] - 6s 49ms/step - loss: 2.2816e-04\n",
            "Epoch 39/50\n",
            "117/117 [==============================] - 5s 45ms/step - loss: 2.3153e-04\n",
            "Epoch 40/50\n",
            " 39/117 [=========>....................] - ETA: 5s - loss: 2.2262e-04"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import yfinance as yf\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from scipy.interpolate import interp1d\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.models import Sequential\n",
        "from xgboost import XGBRegressor\n",
        "import tensorflow as tf\n",
        "from keras.layers import LSTM, Dense\n",
        "\n",
        "# Fetch historical stock prices forHong Kong Index from Yahoo Finance\n",
        "data = yf.download('^HSI', start='2004-01-01', end='2024-01-01')\n",
        "\n",
        "# Ensure 'Date' column is in datetime format\n",
        "data['Date'] = pd.to_datetime(data.index)\n",
        "\n",
        "# Fill missing values in 'Close' column with the previous day's value\n",
        "data['Close'] = data['Close'].fillna(method='ffill')\n",
        "\n",
        "# Feature Engineering\n",
        "data['SMA_50'] = data['Close'].rolling(window=50).mean()  # 50-day Simple Moving Average\n",
        "data['SMA_200'] = data['Close'].rolling(window=200).mean()  # 200-day Simple Moving Average\n",
        "data['EMA_26'] = data['Close'].ewm(span=26, min_periods=0, adjust=False).mean()  # 26-day Exponential Moving Average\n",
        "data['EMA_12'] = data['Close'].ewm(span=12, min_periods=0, adjust=False).mean()  # 12-day Exponential Moving Average\n",
        "data['MACD'] = data['EMA_12'] - data['EMA_26']  # Moving Average Convergence Divergence\n",
        "\n",
        "# Drop rows with NaN values after feature engineering\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Define features and target variable\n",
        "features = ['SMA_50', 'SMA_200', 'EMA_12', 'EMA_26', 'MACD']\n",
        "target = 'Close'\n",
        "X1 = data[features]\n",
        "y1 = data[target]\n",
        "\n",
        "# Split data into train and test sets\n",
        "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize features\n",
        "scaler1 = StandardScaler()\n",
        "X1_train_scaled = scaler1.fit_transform(X1_train)\n",
        "X1_test_scaled = scaler1.transform(X1_test)\n",
        "\n",
        "# Initialize and train XGBoost model\n",
        "model = XGBRegressor(n_estimators=1000, learning_rate=0.01, random_state=42)\n",
        "model.fit(X1_train_scaled, y1_train)\n",
        "\n",
        "# Make predictions\n",
        "XGB_predictions = model.predict(X1_test_scaled)\n",
        "\n",
        "# Build MLP model with increased complexity\n",
        "model2 = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(X1_train_scaled.shape[1],)),\n",
        "    tf.keras.layers.Dense(256, activation='relu'),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "# Compile the model with Adam optimizer and MSE as the loss function\n",
        "model2.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])\n",
        "\n",
        "# Train the model with early stopping\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "history = model2.fit(X1_train_scaled, y1_train, epochs=15, batch_size=32, validation_split=0.2, callbacks=[early_stopping], verbose=1)\n",
        "\n",
        "# Make predictions\n",
        "MLP_predictions = model.predict(X1_test_scaled)\n",
        "\n",
        "# Feature Scaling\n",
        "scaler2 = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler2.fit_transform(data['Close'].values.reshape(-1, 1))\n",
        "\n",
        "# Split data into train and test sets\n",
        "train_size = int(len(scaled_data) * 0.8)\n",
        "train_data, test_data = scaled_data[:train_size], scaled_data[train_size:]\n",
        "\n",
        "# Convert data into sequences\n",
        "def create_sequences(data, seq_length):\n",
        "    X3, y3 = [], []\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X3.append(data[i:i+seq_length])\n",
        "        y3.append(data[i+seq_length])\n",
        "    return np.array(X3), np.array(y3)\n",
        "\n",
        "seq_length = 60  # Sequence length\n",
        "X3_train, y3_train = create_sequences(train_data, seq_length)\n",
        "X3_test, y3_test = create_sequences(test_data, seq_length)\n",
        "\n",
        "# Reshape input data for LSTM\n",
        "X3_train = X3_train.reshape((X3_train.shape[0], X3_train.shape[1], 1))\n",
        "X3_test = X3_test.reshape((X3_test.shape[0], X3_test.shape[1], 1))\n",
        "\n",
        "# Build LSTM model\n",
        "model3 = Sequential()\n",
        "model3.add(LSTM(units=50, return_sequences=True, input_shape=(X3_train.shape[1], 1)))\n",
        "model3.add(LSTM(units=50))\n",
        "model3.add(Dense(units=1))\n",
        "model3.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train LSTM model\n",
        "model3.fit(X3_train, y3_train, epochs=50, batch_size=32)\n",
        "\n",
        "# Predictions\n",
        "LSTM_predictions = model3.predict(X3_test)\n",
        "\n",
        "# Inverse scaling\n",
        "LSTM_predictions = scaler2.inverse_transform(LSTM_predictions)\n",
        "y3_test = scaler2.inverse_transform(y3_test)\n",
        "i = 10000\n",
        "# Interpolate predictions to align them\n",
        "xgb_interp = interp1d(np.arange(len(XGB_predictions)), XGB_predictions, kind='nearest')\n",
        "mlp_interp = interp1d(np.arange(len(MLP_predictions)), MLP_predictions.flatten(), kind='nearest')\n",
        "lstm_interp = interp1d(np.arange(len(LSTM_predictions)), LSTM_predictions.flatten(), kind='nearest')\n",
        "\n",
        "# Calculate the length of the shortest array\n",
        "min_length = min(len(XGB_predictions), len(MLP_predictions), len(LSTM_predictions))\n",
        "\n",
        "# Create ensemble features by concatenating interpolated predictions\n",
        "ensemble_features = np.column_stack((xgb_interp(np.linspace(0, len(XGB_predictions) - 1, min_length)),\n",
        "                                     mlp_interp(np.linspace(0, len(MLP_predictions) - 1, min_length)),\n",
        "                                     lstm_interp(np.linspace(0, len(LSTM_predictions) - 1, min_length))))\n",
        "\n",
        "# Train a linear regression model\n",
        "ensemble_model = LinearRegression()\n",
        "ensemble_model.fit(ensemble_features, y3_test[:min_length])\n",
        "\n",
        "# Predict using the ensemble model\n",
        "ensemble_predictions = ensemble_model.predict(ensemble_features)\n",
        "\n",
        "# Calculate Mean Squared Error for ensemble predictions\n",
        "ensemble_mse = mean_squared_error(y3_test[:min_length], ensemble_predictions)\n",
        "print('Ensemble Mean Squared Error:', ensemble_mse/i)\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "# Assuming y_test and predictions are your actual and predicted values respectively\n",
        "\n",
        "# Mean Absolute Error\n",
        "ensemble_mae = mean_absolute_error(y3_test, ensemble_predictions)\n",
        "print(' Ensemble Mean Absolute Error:', ensemble_mae/i)\n",
        "\n",
        "# Root Mean Squared Error\n",
        "ensemble_rmse = np.sqrt(mean_squared_error(y3_test, ensemble_predictions))\n",
        "print('Ensemble Root Mean Squared Error:', ensemble_rmse/i)\n",
        "\n",
        "# R-squared\n",
        "ensemble_r_squared = r2_score(y3_test, ensemble_predictions)\n",
        "print('Ensemble R-squared:', ensemble_r_squared)\n",
        "\n",
        "# Plotting actual vs ensemble predicted\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(data.index[train_size+seq_length:train_size+seq_length+min_length], y3_test[:min_length], label='Actual', color='blue')\n",
        "plt.plot(data.index[train_size+seq_length:train_size+seq_length+min_length], ensemble_predictions, label='Ensemble Predicted', color='red')\n",
        "plt.title('Actual vs Ensemble Predicted Stock Prices (Hong Kong Index)')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Price')\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "plt.grid(True, linestyle='--', alpha=0.5)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    }
  ]
}